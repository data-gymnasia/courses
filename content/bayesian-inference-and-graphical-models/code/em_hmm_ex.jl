using Distributions, CSV, Printf

# Load observed data
x_samples = CSV.read("hmm_observations.csv")[:,1]


# Gives P(Y_k = 1 | All other variables)
# Inputs
# - x: The vector of (observed) X variables
# - y: The current sample of Y variables
# - Î¸_k: The vector parameters in the form [q, ÏƒÂ²]
# - k: The index of the Y variable of interest
# Outputs
# The probability that Y_k = 1 given all other variables
function conditional_probability(x, y, Î¸_k, k)
    # Extract density parameters
    q, ÏƒÂ² = Î¸_k
    Ïƒ = sqrt(ÏƒÂ²)

    n = length(x)

    ğ’©â‚ = Normal(1, Ïƒ)
    ğ’©â‚€ = Normal(0, Ïƒ)

    if k == 1
        joint = (q*(y[k+1] == 1) + (1-q)*(y[k+1] == 0))*pdf(ğ’©â‚, x[k])

        marginal = joint + (q*(y[k+1] == 0) + (1-q)*(y[k+1] == 1))*pdf(ğ’©â‚€,x[k])
    elseif k == n
        joint = (q*(y[k-1] == 1) + (1-q)*(y[k-1] == 0))*pdf(ğ’©â‚, x[k])

        marginal = joint + (q*(y[k-1] == 0) + (1-q)*(y[k-1] == 1))*pdf(ğ’©â‚€,x[k])

    else
        joint = (q*(y[k-1] == 1) + (1-q)*(y[k-1] == 0))*(q*(y[k+1] == 1) + (1-q)*(y[k+1] == 0))*pdf(ğ’©â‚,x[k])

        marginal = joint + (q*(y[k-1] == 0) + (1-q)*(y[k-1] == 1))*(q*(y[k+1] == 0) + (1-q)*(y[k+1] == 1))*pdf(ğ’©â‚€, x[k])
    end

    return joint/marginal
end


# Performs one single Gibbs sampler iteration of Y
# Inputs
# - x: The vector of (observed) X variables
# - y: The current sample of Y variables
# - Î¸_k: The vector parameters in the form [q, ÏƒÂ²]
# - k: The index of the Y variable of interest
# Outputs
# A Y sample where each index of Y is sampled by conditioning on all other
# variables
function get_single_gibbs_sample(x, y, Î¸_k)
    n = length(x)

    for k = 1:n
        y[k] = rand() < conditional_probability(x, y, Î¸_k, k)
    end

    return y
end

# Returns a sample of Y
# Inputs
# - x: The vector of (observed) X variables
# - Î¸_k: The vector parameters in the form [q, ÏƒÂ²]
# Outputs
# A Y sample where Y ~ P(Y|X = x)
function gibbs_sampler(x, Î¸_k)
    n = length(x)

    y = rand(0:1, n)

    for i = 1:75 # Burn-in period
        y = get_single_gibbs_sample(x, y, Î¸_k)
    end

    return y
end

# Estimates the values of a,b,c (as defined in example) via Monte Carlo
# Inputs
# - x: The vector of (observed) X variables
# - Î¸_k: The vector parameters in the form [q, Ïƒ^2]
# Outputs
# A vector in the form [a,b,c] representing a MC estimate of a,b, and c
function estimate_a_b_c(x, Î¸_k)
    n = length(x)

    # Number of MC samples
    num_samples = 100

    # Estimate sum
    total_a = 0
    total_b = 0
    total_c = 0

    for k = 1:num_samples
        y_samples = gibbs_sampler(x, Î¸_k)
        estimate_a = sum(y_samples[1:(n-1)] .== y_samples[2:n])
        estimate_b = sum(y_samples[1:(n-1)] .!= y_samples[2:n])
        estimate_c = sum((x - y_samples).^2)


        total_a += estimate_a
        total_b += estimate_b
        total_c += estimate_c
    end

    return [total_a, total_b, total_c]./num_samples
end

# Performs EM algorithm to estimate Î¸
# Inputs
# - x: The vector of (observed) X variables
# Outputs
# An estimate of Î¸ = [q, ÏƒÂ²]
function em_algorithm(x)
    # Initialize theta parameter [q, ÏƒÂ²]
    Î¸_k = [0.5, 1]
    @printf("k = 0: [%f, %f]\n", Î¸_k[1], Î¸_k[2])

    num_iterations = 500

    for i = 1:num_iterations
        a, b, c = estimate_a_b_c(x, Î¸_k)
        q_1 = a/(a+b)
        ÏƒÂ²_1 = c/(a+b+1)
        Î¸_k = [q_1, ÏƒÂ²_1]

        @printf("k = %d: [%f, %f]\n", i, Î¸_k[1], Î¸_k[2])
    end

    return Î¸_k
end

em_algorithm(x_samples)
